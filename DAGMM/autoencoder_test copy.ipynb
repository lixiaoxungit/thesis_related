{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network_temp import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = DAGMMnet([10, 1], [10, 1], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data = np.load('./data.npy')\n",
    "# data_set = tf.data.Dataset.from_tensor_slices(raw_data.astype('float32')).shuffle(60000).batch(64)\n",
    "# test_data = list(data_set.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = my_model.compress_net.encoder(raw_data.astype('float32'))\n",
    "# gamma = my_model.estimate_net(z)\n",
    "# gmm_output = GMM(my_model.n_component, z, gamma)\n",
    "# loss_1 = my_model.compress_net.compute_loss(raw_data.astype('float32'))\n",
    "# loss_2 = gmm_output.energy_loss(z)\n",
    "# loss_3 = gmm_output.diag_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 3.7038609981536865 sec\n",
      "Loss: 10.349642753601074\n",
      "Time for epoch 2 is 1.7426187992095947 sec\n",
      "Loss: 6.797159671783447\n",
      "Time for epoch 3 is 1.667452096939087 sec\n",
      "Loss: 4.734500885009766\n",
      "Time for epoch 4 is 1.6533734798431396 sec\n",
      "Loss: 3.285245895385742\n",
      "Time for epoch 5 is 1.6880860328674316 sec\n",
      "Loss: 2.3154280185699463\n",
      "Time for epoch 6 is 1.638641595840454 sec\n",
      "Loss: 1.74616539478302\n",
      "Time for epoch 7 is 1.6478726863861084 sec\n",
      "Loss: 1.4319902658462524\n",
      "Time for epoch 8 is 1.6147253513336182 sec\n",
      "Loss: 1.2572752237319946\n",
      "Time for epoch 9 is 1.6059072017669678 sec\n",
      "Loss: 1.1474117040634155\n",
      "Time for epoch 10 is 1.7951042652130127 sec\n",
      "Loss: 1.051720380783081\n",
      "Time for epoch 11 is 1.7653183937072754 sec\n",
      "Loss: 0.9461291432380676\n",
      "Time for epoch 12 is 1.6474804878234863 sec\n",
      "Loss: 0.826828122138977\n",
      "Time for epoch 13 is 1.6671721935272217 sec\n",
      "Loss: 0.7030162215232849\n",
      "Time for epoch 14 is 1.7838678359985352 sec\n",
      "Loss: 0.5835632085800171\n",
      "Time for epoch 15 is 1.5979375839233398 sec\n",
      "Loss: 0.48721843957901\n",
      "Time for epoch 16 is 1.6945791244506836 sec\n",
      "Loss: 0.41422775387763977\n",
      "Time for epoch 17 is 1.6763386726379395 sec\n",
      "Loss: 0.3627921938896179\n",
      "Time for epoch 18 is 1.6551125049591064 sec\n",
      "Loss: 0.3285064697265625\n",
      "Time for epoch 19 is 1.6079699993133545 sec\n",
      "Loss: 0.30657878518104553\n",
      "Time for epoch 20 is 1.6181755065917969 sec\n",
      "Loss: 0.3071536123752594\n",
      "Time for epoch 21 is 1.6099185943603516 sec\n",
      "Loss: 0.27370086312294006\n",
      "Time for epoch 22 is 1.577317476272583 sec\n",
      "Loss: 0.26270678639411926\n",
      "Time for epoch 23 is 1.621232271194458 sec\n",
      "Loss: 0.25496286153793335\n",
      "Time for epoch 24 is 1.6134464740753174 sec\n",
      "Loss: 0.2415422797203064\n",
      "Time for epoch 25 is 1.6004228591918945 sec\n",
      "Loss: 0.2329336702823639\n",
      "Time for epoch 26 is 1.6076579093933105 sec\n",
      "Loss: 0.23016709089279175\n",
      "Time for epoch 27 is 1.5952048301696777 sec\n",
      "Loss: 0.22035472095012665\n",
      "Time for epoch 28 is 1.6000726222991943 sec\n",
      "Loss: 0.21345606446266174\n",
      "Time for epoch 29 is 1.6089539527893066 sec\n",
      "Loss: 0.20702536404132843\n",
      "Time for epoch 30 is 1.5821614265441895 sec\n",
      "Loss: 0.2017018049955368\n",
      "Time for epoch 31 is 1.5944547653198242 sec\n",
      "Loss: 0.19394855201244354\n",
      "Time for epoch 32 is 1.584528923034668 sec\n",
      "Loss: 0.18715804815292358\n",
      "Time for epoch 33 is 1.5941441059112549 sec\n",
      "Loss: 0.17954891920089722\n",
      "Time for epoch 34 is 1.611163854598999 sec\n",
      "Loss: 0.17267318069934845\n",
      "Time for epoch 35 is 1.5880067348480225 sec\n",
      "Loss: 0.16234752535820007\n",
      "Time for epoch 36 is 1.6807472705841064 sec\n",
      "Loss: 0.1565849334001541\n",
      "Time for epoch 37 is 1.7205636501312256 sec\n",
      "Loss: 0.14635513722896576\n",
      "Time for epoch 38 is 1.640120267868042 sec\n",
      "Loss: 0.13914979994297028\n",
      "Time for epoch 39 is 1.8835365772247314 sec\n",
      "Loss: 0.132448211312294\n",
      "Time for epoch 40 is 1.8235046863555908 sec\n",
      "Loss: 0.12915192544460297\n",
      "Time for epoch 41 is 1.7436165809631348 sec\n",
      "Loss: 0.12405893206596375\n",
      "Time for epoch 42 is 1.6784777641296387 sec\n",
      "Loss: 0.1162826269865036\n",
      "Time for epoch 43 is 1.6522715091705322 sec\n",
      "Loss: 0.11228381842374802\n",
      "Time for epoch 44 is 1.6059606075286865 sec\n",
      "Loss: 0.11364702135324478\n",
      "Time for epoch 45 is 1.5954086780548096 sec\n",
      "Loss: 0.11491129547357559\n",
      "Time for epoch 46 is 1.592437505722046 sec\n",
      "Loss: 0.10721902549266815\n",
      "Time for epoch 47 is 1.5896153450012207 sec\n",
      "Loss: 0.09992042183876038\n",
      "Time for epoch 48 is 1.5910391807556152 sec\n",
      "Loss: 0.10026323795318604\n",
      "Time for epoch 49 is 1.5942625999450684 sec\n",
      "Loss: 0.10031962394714355\n",
      "Time for epoch 50 is 1.5960843563079834 sec\n",
      "Loss: 0.09481582045555115\n",
      "Time for epoch 51 is 1.5871894359588623 sec\n",
      "Loss: 0.09344477206468582\n",
      "Time for epoch 52 is 1.5879812240600586 sec\n",
      "Loss: 0.0942307636141777\n",
      "Time for epoch 53 is 1.582634687423706 sec\n",
      "Loss: 0.0914192944765091\n",
      "Time for epoch 54 is 1.592148780822754 sec\n",
      "Loss: 0.08949355781078339\n",
      "Time for epoch 55 is 1.5869686603546143 sec\n",
      "Loss: 0.088338702917099\n"
     ]
    }
   ],
   "source": [
    "raw_data = np.load('./data.npy')\n",
    "hidden = np.load('./hidden.npy')\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "data_set = tf.data.Dataset.from_tensor_slices(raw_data.astype('float32')).shuffle(60000).batch(256)\n",
    "train(data_set, my_model, 55, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 3.5282483100891113 sec\n",
      "Loss: 20.390352249145508\n",
      "Time for epoch 2 is 3.5711042881011963 sec\n",
      "Loss: 20.39023208618164\n",
      "Time for epoch 3 is 3.5658836364746094 sec\n",
      "Loss: 20.271377563476562\n",
      "Time for epoch 4 is 3.637331962585449 sec\n",
      "Loss: 20.2530460357666\n",
      "Time for epoch 5 is 3.6395559310913086 sec\n",
      "Loss: 20.305356979370117\n",
      "Time for epoch 6 is 2.7921524047851562 sec\n",
      "Loss: 20.19831085205078\n",
      "Time for epoch 7 is 2.2343411445617676 sec\n",
      "Loss: 20.167314529418945\n",
      "Time for epoch 8 is 2.209611654281616 sec\n",
      "Loss: 20.087675094604492\n",
      "Time for epoch 9 is 2.207282304763794 sec\n",
      "Loss: 20.08258628845215\n",
      "Time for epoch 10 is 2.2033913135528564 sec\n",
      "Loss: 19.94313621520996\n",
      "Time for epoch 11 is 2.2117671966552734 sec\n",
      "Loss: 19.831632614135742\n",
      "Time for epoch 12 is 2.2074084281921387 sec\n",
      "Loss: 19.574953079223633\n",
      "Time for epoch 13 is 2.2034571170806885 sec\n",
      "Loss: 19.218585968017578\n",
      "Time for epoch 14 is 2.2185938358306885 sec\n",
      "Loss: 18.67519760131836\n",
      "Time for epoch 15 is 2.1873507499694824 sec\n",
      "Loss: 17.81826400756836\n",
      "Time for epoch 16 is 2.3329145908355713 sec\n",
      "Loss: 15.363348960876465\n",
      "Time for epoch 17 is 2.2501349449157715 sec\n",
      "Loss: 13.40102481842041\n",
      "Time for epoch 18 is 2.212642192840576 sec\n",
      "Loss: 11.922873497009277\n",
      "Time for epoch 19 is 2.2319722175598145 sec\n",
      "Loss: 10.93225383758545\n",
      "Time for epoch 20 is 2.3191428184509277 sec\n",
      "Loss: 10.228052139282227\n",
      "Time for epoch 21 is 2.248595714569092 sec\n",
      "Loss: 9.556462287902832\n",
      "Time for epoch 22 is 2.167102575302124 sec\n",
      "Loss: 8.522221565246582\n",
      "Time for epoch 23 is 2.1686320304870605 sec\n",
      "Loss: 7.322887897491455\n",
      "Time for epoch 24 is 2.159376621246338 sec\n",
      "Loss: 6.3888092041015625\n",
      "Time for epoch 25 is 2.1664505004882812 sec\n",
      "Loss: 5.6389055252075195\n",
      "Time for epoch 26 is 2.160705089569092 sec\n",
      "Loss: 5.006752014160156\n",
      "Time for epoch 27 is 2.161390542984009 sec\n",
      "Loss: 4.506702423095703\n",
      "Time for epoch 28 is 2.1861300468444824 sec\n",
      "Loss: 4.114628314971924\n",
      "Time for epoch 29 is 2.1936757564544678 sec\n",
      "Loss: 3.7950360774993896\n",
      "Time for epoch 30 is 2.1852478981018066 sec\n",
      "Loss: 3.553675889968872\n",
      "Time for epoch 31 is 2.2475411891937256 sec\n",
      "Loss: 3.367144823074341\n",
      "Time for epoch 32 is 2.2701127529144287 sec\n",
      "Loss: 3.2207112312316895\n",
      "Time for epoch 33 is 2.2849345207214355 sec\n",
      "Loss: 3.097228527069092\n",
      "Time for epoch 34 is 2.175163984298706 sec\n",
      "Loss: 3.012655735015869\n",
      "Time for epoch 35 is 2.1733782291412354 sec\n",
      "Loss: 2.8737449645996094\n",
      "Time for epoch 36 is 2.1650688648223877 sec\n",
      "Loss: 2.750239610671997\n",
      "Time for epoch 37 is 2.1733744144439697 sec\n",
      "Loss: 2.641230821609497\n",
      "Time for epoch 38 is 2.2367265224456787 sec\n",
      "Loss: 2.5069665908813477\n",
      "Time for epoch 39 is 2.177464008331299 sec\n",
      "Loss: 2.3612029552459717\n",
      "Time for epoch 40 is 2.1701467037200928 sec\n",
      "Loss: 2.1457455158233643\n",
      "Time for epoch 41 is 2.171151876449585 sec\n",
      "Loss: 1.9188628196716309\n",
      "Time for epoch 42 is 2.181332588195801 sec\n",
      "Loss: 1.6945149898529053\n",
      "Time for epoch 43 is 2.1555817127227783 sec\n",
      "Loss: 1.4848592281341553\n",
      "Time for epoch 44 is 2.1609768867492676 sec\n",
      "Loss: 1.3079155683517456\n",
      "Time for epoch 45 is 2.1738224029541016 sec\n",
      "Loss: 1.1707874536514282\n",
      "Time for epoch 46 is 2.180950403213501 sec\n",
      "Loss: 1.0426578521728516\n",
      "Time for epoch 47 is 2.1700165271759033 sec\n",
      "Loss: 0.9340470433235168\n",
      "Time for epoch 48 is 2.178392171859741 sec\n",
      "Loss: 0.8549363613128662\n",
      "Time for epoch 49 is 2.154832601547241 sec\n",
      "Loss: 0.805842936038971\n",
      "Time for epoch 50 is 2.164863109588623 sec\n",
      "Loss: 0.7148367166519165\n"
     ]
    }
   ],
   "source": [
    "train(data_set, my_model, 50, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dagm_mnet is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result, z = my_model(raw_data)\n",
    "#  如果要使用predict，需要重新设置batchsize\n",
    "#  result_, z_ = my_model.predict(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = np.exp(-result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86130553"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit[:50000].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14636621"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit[50000:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer compression_net is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = my_model.compress_net(raw_data)\n",
    "gamma = my_model.estimate_net(z)\n",
    "gmm_output = GMM(z, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_new = my_model.compress_net.decoder(my_model.compress_net.encoder(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\n",
       "array([[[ 1.3899577 , -0.15162157,  0.11579624],\n",
       "        [-0.15162157,  3.613577  , -0.3563622 ],\n",
       "        [ 0.11579624, -0.3563622 ,  0.2695724 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_output.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.7546778 , 0.31944233, 0.84785765]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_output.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1e39bb509cfb5ccd31f70cb06d48c419b771ae25efbac49dcd3f7bd695e8586"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
