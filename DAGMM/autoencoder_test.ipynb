{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = DAGMMnet([10, 1], [10, 1], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data = np.load('./data.npy')\n",
    "# data_set = tf.data.Dataset.from_tensor_slices(raw_data.astype('float32')).shuffle(60000).batch(64)\n",
    "# test_data = list(data_set.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = my_model.compress_net.encoder(raw_data.astype('float32'))\n",
    "# gamma = my_model.estimate_net(z)\n",
    "# gmm_output = GMM(my_model.n_component, z, gamma)\n",
    "# loss_1 = my_model.compress_net.compute_loss(raw_data.astype('float32'))\n",
    "# loss_2 = gmm_output.energy_loss(z)\n",
    "# loss_3 = gmm_output.diag_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 7.915375709533691 sec\n",
      "Loss: 22.728443145751953\n",
      "Time for epoch 2 is 3.6131818294525146 sec\n",
      "Loss: 21.893142700195312\n",
      "Time for epoch 3 is 3.619230270385742 sec\n",
      "Loss: 21.12427520751953\n",
      "Time for epoch 4 is 3.6185035705566406 sec\n",
      "Loss: 20.797760009765625\n",
      "Time for epoch 5 is 3.641319751739502 sec\n",
      "Loss: 20.517419815063477\n"
     ]
    }
   ],
   "source": [
    "raw_data = np.load('./data.npy')\n",
    "hidden = np.load('./hidden.npy')\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "data_set = tf.data.Dataset.from_tensor_slices(raw_data.astype('float32')).shuffle(60000).batch(256)\n",
    "train(data_set, my_model, 5, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 3.5282483100891113 sec\n",
      "Loss: 20.390352249145508\n",
      "Time for epoch 2 is 3.5711042881011963 sec\n",
      "Loss: 20.39023208618164\n",
      "Time for epoch 3 is 3.5658836364746094 sec\n",
      "Loss: 20.271377563476562\n",
      "Time for epoch 4 is 3.637331962585449 sec\n",
      "Loss: 20.2530460357666\n",
      "Time for epoch 5 is 3.6395559310913086 sec\n",
      "Loss: 20.305356979370117\n",
      "Time for epoch 6 is 2.7921524047851562 sec\n",
      "Loss: 20.19831085205078\n",
      "Time for epoch 7 is 2.2343411445617676 sec\n",
      "Loss: 20.167314529418945\n",
      "Time for epoch 8 is 2.209611654281616 sec\n",
      "Loss: 20.087675094604492\n",
      "Time for epoch 9 is 2.207282304763794 sec\n",
      "Loss: 20.08258628845215\n",
      "Time for epoch 10 is 2.2033913135528564 sec\n",
      "Loss: 19.94313621520996\n",
      "Time for epoch 11 is 2.2117671966552734 sec\n",
      "Loss: 19.831632614135742\n",
      "Time for epoch 12 is 2.2074084281921387 sec\n",
      "Loss: 19.574953079223633\n",
      "Time for epoch 13 is 2.2034571170806885 sec\n",
      "Loss: 19.218585968017578\n",
      "Time for epoch 14 is 2.2185938358306885 sec\n",
      "Loss: 18.67519760131836\n",
      "Time for epoch 15 is 2.1873507499694824 sec\n",
      "Loss: 17.81826400756836\n",
      "Time for epoch 16 is 2.3329145908355713 sec\n",
      "Loss: 15.363348960876465\n",
      "Time for epoch 17 is 2.2501349449157715 sec\n",
      "Loss: 13.40102481842041\n",
      "Time for epoch 18 is 2.212642192840576 sec\n",
      "Loss: 11.922873497009277\n",
      "Time for epoch 19 is 2.2319722175598145 sec\n",
      "Loss: 10.93225383758545\n",
      "Time for epoch 20 is 2.3191428184509277 sec\n",
      "Loss: 10.228052139282227\n",
      "Time for epoch 21 is 2.248595714569092 sec\n",
      "Loss: 9.556462287902832\n",
      "Time for epoch 22 is 2.167102575302124 sec\n",
      "Loss: 8.522221565246582\n",
      "Time for epoch 23 is 2.1686320304870605 sec\n",
      "Loss: 7.322887897491455\n",
      "Time for epoch 24 is 2.159376621246338 sec\n",
      "Loss: 6.3888092041015625\n",
      "Time for epoch 25 is 2.1664505004882812 sec\n",
      "Loss: 5.6389055252075195\n",
      "Time for epoch 26 is 2.160705089569092 sec\n",
      "Loss: 5.006752014160156\n",
      "Time for epoch 27 is 2.161390542984009 sec\n",
      "Loss: 4.506702423095703\n",
      "Time for epoch 28 is 2.1861300468444824 sec\n",
      "Loss: 4.114628314971924\n",
      "Time for epoch 29 is 2.1936757564544678 sec\n",
      "Loss: 3.7950360774993896\n",
      "Time for epoch 30 is 2.1852478981018066 sec\n",
      "Loss: 3.553675889968872\n",
      "Time for epoch 31 is 2.2475411891937256 sec\n",
      "Loss: 3.367144823074341\n",
      "Time for epoch 32 is 2.2701127529144287 sec\n",
      "Loss: 3.2207112312316895\n",
      "Time for epoch 33 is 2.2849345207214355 sec\n",
      "Loss: 3.097228527069092\n",
      "Time for epoch 34 is 2.175163984298706 sec\n",
      "Loss: 3.012655735015869\n",
      "Time for epoch 35 is 2.1733782291412354 sec\n",
      "Loss: 2.8737449645996094\n",
      "Time for epoch 36 is 2.1650688648223877 sec\n",
      "Loss: 2.750239610671997\n",
      "Time for epoch 37 is 2.1733744144439697 sec\n",
      "Loss: 2.641230821609497\n",
      "Time for epoch 38 is 2.2367265224456787 sec\n",
      "Loss: 2.5069665908813477\n",
      "Time for epoch 39 is 2.177464008331299 sec\n",
      "Loss: 2.3612029552459717\n",
      "Time for epoch 40 is 2.1701467037200928 sec\n",
      "Loss: 2.1457455158233643\n",
      "Time for epoch 41 is 2.171151876449585 sec\n",
      "Loss: 1.9188628196716309\n",
      "Time for epoch 42 is 2.181332588195801 sec\n",
      "Loss: 1.6945149898529053\n",
      "Time for epoch 43 is 2.1555817127227783 sec\n",
      "Loss: 1.4848592281341553\n",
      "Time for epoch 44 is 2.1609768867492676 sec\n",
      "Loss: 1.3079155683517456\n",
      "Time for epoch 45 is 2.1738224029541016 sec\n",
      "Loss: 1.1707874536514282\n",
      "Time for epoch 46 is 2.180950403213501 sec\n",
      "Loss: 1.0426578521728516\n",
      "Time for epoch 47 is 2.1700165271759033 sec\n",
      "Loss: 0.9340470433235168\n",
      "Time for epoch 48 is 2.178392171859741 sec\n",
      "Loss: 0.8549363613128662\n",
      "Time for epoch 49 is 2.154832601547241 sec\n",
      "Loss: 0.805842936038971\n",
      "Time for epoch 50 is 2.164863109588623 sec\n",
      "Loss: 0.7148367166519165\n"
     ]
    }
   ],
   "source": [
    "train(data_set, my_model, 50, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dagm_mnet is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result, z = my_model(raw_data)\n",
    "#  如果要使用predict，需要重新设置batchsize\n",
    "#  result_, z_ = my_model.predict(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = np.exp(-result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046055343"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit[:50000].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0073861815"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit[50000:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer compression_net is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = my_model.compress_net(raw_data)\n",
    "gamma = my_model.estimate_net(z)\n",
    "gmm_output = GMM(z, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_new = my_model.compress_net.decoder(my_model.compress_net.encoder(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\n",
       "array([[[ 1.3899577 , -0.15162157,  0.11579624],\n",
       "        [-0.15162157,  3.613577  , -0.3563622 ],\n",
       "        [ 0.11579624, -0.3563622 ,  0.2695724 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_output.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.7546778 , 0.31944233, 0.84785765]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_output.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1e39bb509cfb5ccd31f70cb06d48c419b771ae25efbac49dcd3f7bd695e8586"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
