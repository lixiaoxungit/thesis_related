{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# autoencoder\n",
    "\n",
    "\n",
    "class CompressionNet(Layer):\n",
    "    def __init__(self, hidden_layer_size, input_size):\n",
    "        # latent_layer_size: list of hidden layer size\n",
    "        # input_size: int\n",
    "        super(CompressionNet, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.latent_dim = hidden_layer_size[-1]\n",
    "        self.input_size = input_size\n",
    "        self.encoder = tf.keras.Sequential()\n",
    "        self.encoder.add(tf.keras.Input(shape=(self.input_size,)))\n",
    "        for size in self.hidden_layer_size[:-1]:\n",
    "            self.encoder.add(tf.keras.layers.Dense(size, activation='tanh'))\n",
    "        self.encoder.add(tf.keras.layers.Dense(self.latent_dim))\n",
    "        self.decoder = tf.keras.Sequential()\n",
    "        self.decoder.add(tf.keras.Input(shape=(self.latent_dim,)))\n",
    "        for size in self.hidden_layer_size[::-1][1:]:\n",
    "            self.decoder.add(tf.keras.layers.Dense(size, activation='tanh'))\n",
    "        self.decoder.add(tf.keras.layers.Dense(self.input_size))\n",
    "\n",
    "    def reconstruction_error_feature(self, x):\n",
    "        # This is a part of estimation net input\n",
    "        # In original paper, relative Euclidean distance and\n",
    "        # cosine similarity is considered\n",
    "        def euclid_norm(x):\n",
    "            return tf.sqrt(tf.reduce_sum(tf.square(x), axis=1))\n",
    "        hidden_state = self.encoder(x)\n",
    "        x_new = self.decoder(hidden_state)\n",
    "        # relative Euclidean distance\n",
    "        loss_m = euclid_norm(x - x_new) / euclid_norm(x)\n",
    "        # cosine similarity\n",
    "        loss_c = tf.reduce_sum(x * x_new, axis=1) / \\\n",
    "            (euclid_norm(x) * euclid_norm(x_new))\n",
    "        return tf.concat([loss_m[:, None], loss_c[:, None]], axis=1)\n",
    "\n",
    "    def compute_loss(self, x):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        hidden_state = self.encoder(x)\n",
    "        x_new = self.decoder(hidden_state)\n",
    "        loss = mse(x, x_new)\n",
    "        return loss\n",
    "\n",
    "    def call(self, inputs):\n",
    "        hidden_state = self.encoder(inputs)\n",
    "        reconstruction_error = self.reconstruction_error_feature(inputs)\n",
    "        features = tf.concat([hidden_state, reconstruction_error], axis=1)\n",
    "        return features\n",
    "\n",
    "\n",
    "class EstimationNet(Layer):\n",
    "    def __init__(self, hidden_layer_size, input_size):\n",
    "        # latent_layer_size: list of hidden layer size\n",
    "        # input_size: int\n",
    "        super(EstimationNet, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.input_size = input_size\n",
    "        self.inference = tf.keras.Sequential()\n",
    "        self.inference.add(tf.keras.Input(shape=(self.input_size,)))\n",
    "        for size in self.hidden_layer_size[:-1]:\n",
    "            self.inference.add(tf.keras.layers.Dense(size, activation='tanh'))\n",
    "            self.inference.add(tf.keras.layers.Dropout(0.2))\n",
    "        self.inference.add(tf.keras.layers.Dense(\n",
    "            self.hidden_layer_size[-1], activation='softmax'))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.inference(inputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class GMM:\n",
    "    def __init__(self, z, gamma):\n",
    "        # z : tf.Tensor, shape (n_samples, n_features)\n",
    "        # gamma : tf.Tensor, shape (n_samples, n_component)\n",
    "        gamma_sum = tf.reduce_sum(gamma, axis=0)  # shape (n_component, )\n",
    "        self.phi = tf.reduce_mean(gamma, axis=0)  # shape (n_component, )\n",
    "        # shape (n_component, n_features)\n",
    "        self.mu = tf.matmul(gamma, z, transpose_a=True) / gamma_sum[:, None]\n",
    "        z_centered = tf.sqrt(gamma[:, :, None]) * \\\n",
    "            (z[:, None, :] - self.mu[None, :, :])\n",
    "        self.sigma = tf.einsum(\n",
    "            'ikl,ikm->klm', z_centered, z_centered) / gamma_sum[:, None, None]\n",
    "        # Calculate a cholesky decomposition of covariance in advance\n",
    "        n_features = z.shape[1]\n",
    "        min_vals = tf.linalg.diag(\n",
    "            tf.ones(n_features, dtype=tf.float32)) * 1e-6\n",
    "        self.L = tf.linalg.cholesky(self.sigma + min_vals[None, :, :])\n",
    "\n",
    "    def compute_prob(self, z):\n",
    "        # Instead of inverse covariance matrix, exploit cholesky decomposition\n",
    "        # for stability of calculation.\n",
    "        # shape (n_samples, n_component, n_features)\n",
    "        z_centered = z[:, None, :] - self.mu[None, :, :]\n",
    "        v = tf.linalg.triangular_solve(\n",
    "            self.L, tf.transpose(z_centered, [1, 2, 0]))\n",
    "\n",
    "        # log(det(Sigma)) = 2 * sum[log(diag(L))]\n",
    "        log_det_sigma = 2.0 * \\\n",
    "            tf.reduce_sum(tf.math.log(tf.linalg.diag_part(self.L)), axis=1)\n",
    "\n",
    "        # To calculate prob, use \"log-sum-exp\" (different from orginal paper)\n",
    "        d = z.shape[1]\n",
    "        logits = tf.math.log(self.phi[:, None]) - 0.5 * (tf.reduce_sum(\n",
    "            tf.square(v), axis=1) + d * tf.math.log(2.0 * np.pi) +\n",
    "            log_det_sigma[:, None])\n",
    "        prob = - tf.reduce_logsumexp(logits, axis=0)\n",
    "        return prob\n",
    "\n",
    "    def energy_loss(self, z):\n",
    "        return tf.reduce_mean(self.compute_prob(z))\n",
    "\n",
    "    def diag_loss(self):\n",
    "        diag_loss = tf.reduce_sum(\n",
    "            tf.divide(1, tf.linalg.diag_part(self.sigma)))\n",
    "        return diag_loss\n",
    "\n",
    "\n",
    "class DAGMMnet(Model):\n",
    "    def __init__(self, compress_hidden_layer,\n",
    "                 estimate_hidden_layer, input_size):\n",
    "        super(DAGMMnet, self).__init__()\n",
    "        self.compress_hidden_layer = compress_hidden_layer\n",
    "        self.latent_dim = compress_hidden_layer[-1]\n",
    "        self.estimate_hidden_layer = estimate_hidden_layer\n",
    "        self.input_size = input_size\n",
    "        self.compress_net = CompressionNet(\n",
    "            self.compress_hidden_layer, self.input_size)\n",
    "        # +2 means reconstruct features\n",
    "        self.estimate_net = EstimationNet(\n",
    "            self.estimate_hidden_layer, self.latent_dim + 2)\n",
    "\n",
    "    def compute_loss(self, x):\n",
    "        lambda1 = 0.1\n",
    "        lambda2 = 0.005\n",
    "        z = self.compress_net(x)\n",
    "        gamma = self.estimate_net(z)\n",
    "        gmm_output = GMM(z, gamma)\n",
    "        loss_1 = self.compress_net.compute_loss(x)\n",
    "        loss_2 = gmm_output.energy_loss(z)\n",
    "        loss_3 = gmm_output.diag_loss()\n",
    "        return loss_1 + lambda1 * loss_2 + lambda2 * loss_3\n",
    "\n",
    "    def call(self, x):\n",
    "        z = self.compress_net(x)\n",
    "        gamma = self.estimate_net(z)\n",
    "        gmm_output = GMM(z, gamma)\n",
    "        result = gmm_output.compute_prob(z)\n",
    "        return result, z\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "    with tf.GradientTape() as t:\n",
    "        loss = model.compute_loss(x)\n",
    "        gradient = t.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "\n",
    "\n",
    "def train(dataset, model, epochs, optimizer):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for data in dataset:\n",
    "            train_step(model, data, optimizer)\n",
    "        batch_num = 0\n",
    "        total_loss = 0\n",
    "        for data in dataset:\n",
    "            batch_num += 1\n",
    "            total_loss += model.compute_loss(data)\n",
    "        print('Time for epoch {} is {} sec'.format(\n",
    "            epoch + 1, time.time() - start))\n",
    "        print('Loss: {}'.format(total_loss/batch_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extract = tf.keras.models.load_model('./feature_extract.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.load('./x.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(-1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x_tran = feature_extract(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = DAGMMnet([10, 1], [10, 1], 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer compression_net is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Time for epoch 1 is 4.094263553619385 sec\n",
      "Loss: 5.540744781494141\n",
      "Time for epoch 2 is 0.33045458793640137 sec\n",
      "Loss: 3.5074245929718018\n",
      "Time for epoch 3 is 0.3123800754547119 sec\n",
      "Loss: 2.615748882293701\n",
      "Time for epoch 4 is 0.2935361862182617 sec\n",
      "Loss: 1.9973329305648804\n",
      "Time for epoch 5 is 0.3161029815673828 sec\n",
      "Loss: 1.7395750284194946\n",
      "Time for epoch 6 is 0.2968461513519287 sec\n",
      "Loss: 1.576899766921997\n",
      "Time for epoch 7 is 0.31420183181762695 sec\n",
      "Loss: 1.4559098482131958\n",
      "Time for epoch 8 is 0.30274438858032227 sec\n",
      "Loss: 1.271357536315918\n",
      "Time for epoch 9 is 0.30103254318237305 sec\n",
      "Loss: 1.1675444841384888\n",
      "Time for epoch 10 is 0.3015422821044922 sec\n",
      "Loss: 1.0664077997207642\n",
      "Time for epoch 11 is 0.29855895042419434 sec\n",
      "Loss: 0.9848252534866333\n",
      "Time for epoch 12 is 0.29694223403930664 sec\n",
      "Loss: 0.9040612578392029\n",
      "Time for epoch 13 is 0.31365180015563965 sec\n",
      "Loss: 0.8509660959243774\n",
      "Time for epoch 14 is 0.29536890983581543 sec\n",
      "Loss: 0.7914185523986816\n",
      "Time for epoch 15 is 0.30537939071655273 sec\n",
      "Loss: 0.7484040260314941\n",
      "Time for epoch 16 is 0.2986748218536377 sec\n",
      "Loss: 0.7324320673942566\n",
      "Time for epoch 17 is 0.3496129512786865 sec\n",
      "Loss: 0.6879263520240784\n",
      "Time for epoch 18 is 0.30209946632385254 sec\n",
      "Loss: 0.6543629765510559\n",
      "Time for epoch 19 is 0.29710936546325684 sec\n",
      "Loss: 0.6367711424827576\n",
      "Time for epoch 20 is 0.30133795738220215 sec\n",
      "Loss: 0.6144601106643677\n",
      "Time for epoch 21 is 0.29463696479797363 sec\n",
      "Loss: 0.5955042243003845\n",
      "Time for epoch 22 is 0.29635143280029297 sec\n",
      "Loss: 0.5826679468154907\n",
      "Time for epoch 23 is 0.31246423721313477 sec\n",
      "Loss: 0.5718739032745361\n",
      "Time for epoch 24 is 0.2991950511932373 sec\n",
      "Loss: 0.5576114654541016\n",
      "Time for epoch 25 is 0.29436779022216797 sec\n",
      "Loss: 0.545759379863739\n",
      "Time for epoch 26 is 0.3008561134338379 sec\n",
      "Loss: 0.5331904888153076\n",
      "Time for epoch 27 is 0.2952382564544678 sec\n",
      "Loss: 0.5302861928939819\n",
      "Time for epoch 28 is 0.303422212600708 sec\n",
      "Loss: 0.5161916613578796\n",
      "Time for epoch 29 is 0.3081231117248535 sec\n",
      "Loss: 0.5069366693496704\n",
      "Time for epoch 30 is 0.2965106964111328 sec\n",
      "Loss: 0.4959200918674469\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "data_set = tf.data.Dataset.from_tensor_slices(train_x).shuffle(2337).batch(128)\n",
    "train(data_set, my_model, 30, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dagm_mnet is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result, _ = my_model(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = np.exp(-result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.load('./y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = (label == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8578598"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit[label].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0722003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit[~label].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.050097"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1e39bb509cfb5ccd31f70cb06d48c419b771ae25efbac49dcd3f7bd695e8586"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
